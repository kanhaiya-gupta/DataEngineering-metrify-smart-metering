[tool:pytest]
# Pytest configuration for Metrify Smart Metering Data Pipeline

# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Markers
markers =
    unit: Unit tests - fast, isolated tests for individual components
    integration: Integration tests - tests for component interactions
    e2e: End-to-end tests - complete user workflow tests
    performance: Performance tests - load tests, stress tests, benchmarks
    slow: Slow tests - tests that take longer than 1 second
    database: Tests requiring database connection
    kafka: Tests requiring Kafka
    s3: Tests requiring S3
    external: Tests requiring external services
    smoke: Smoke tests - basic functionality verification
    regression: Regression tests - prevent previously fixed bugs

# Test execution
addopts = 
    -v
    --tb=short
    --strict-markers
    --disable-warnings
    --color=yes
    --durations=10
    --maxfail=5
    --html=reports/test_report.html
    --self-contained-html
    --junitxml=reports/junit.xml

# Filtering
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::UserWarning:sqlalchemy.*

# Logging
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = reports/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d %(funcName)s(): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Coverage
addopts = --cov=src --cov=presentation --cov-report=html --cov-report=term-missing --cov-report=xml

# Async support
asyncio_mode = auto

# Timeout
timeout = 300
timeout_method = thread
