# Docker Compose for Metrify Smart Metering Data Pipeline
# Production environment with external services

version: '3.8'

services:
  # Metrify API Service
  api:
    build:
      context: ../..
      dockerfile: infrastructure/docker/Dockerfile.api
    container_name: metrify-api
    environment:
      - METRIFY_ENVIRONMENT=production
      - DB_HOST=${DB_HOST}
      - DB_PORT=5432
      - DB_NAME=${DB_NAME}
      - DB_USERNAME=${DB_USERNAME}
      - DB_PASSWORD=${DB_PASSWORD}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - KAFKA_SECURITY_PROTOCOL=${KAFKA_SECURITY_PROTOCOL}
      - KAFKA_SASL_USERNAME=${KAFKA_SASL_USERNAME}
      - KAFKA_SASL_PASSWORD=${KAFKA_SASL_PASSWORD}
      - KAFKA_CONSUMER_GROUP=${KAFKA_CONSUMER_GROUP}
      - S3_REGION=${S3_REGION}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
      - SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
      - SNOWFLAKE_USERNAME=${SNOWFLAKE_USERNAME}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - REDIS_URL=${REDIS_URL}
    ports:
      - "8000:8000"
    volumes:
      - ../../logs:/app/logs
    restart: unless-stopped
    networks:
      - metrify-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Metrify Ingestion Worker
  ingestion-worker:
    build:
      context: ../..
      dockerfile: infrastructure/docker/Dockerfile.worker
    container_name: metrify-ingestion-worker
    environment:
      - METRIFY_ENVIRONMENT=production
      - DB_HOST=${DB_HOST}
      - DB_PORT=5432
      - DB_NAME=${DB_NAME}
      - DB_USERNAME=${DB_USERNAME}
      - DB_PASSWORD=${DB_PASSWORD}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - KAFKA_SECURITY_PROTOCOL=${KAFKA_SECURITY_PROTOCOL}
      - KAFKA_SASL_USERNAME=${KAFKA_SASL_USERNAME}
      - KAFKA_SASL_PASSWORD=${KAFKA_SASL_PASSWORD}
      - KAFKA_CONSUMER_GROUP=${KAFKA_CONSUMER_GROUP}-ingestion
      - S3_REGION=${S3_REGION}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
      - REDIS_URL=${REDIS_URL}
    volumes:
      - ../../logs:/app/logs
    restart: unless-stopped
    networks:
      - metrify-network
    command: ["python", "-m", "src.presentation.workers.ingestion_worker"]
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Metrify Processing Worker
  processing-worker:
    build:
      context: ../..
      dockerfile: infrastructure/docker/Dockerfile.worker
    container_name: metrify-processing-worker
    environment:
      - METRIFY_ENVIRONMENT=production
      - DB_HOST=${DB_HOST}
      - DB_PORT=5432
      - DB_NAME=${DB_NAME}
      - DB_USERNAME=${DB_USERNAME}
      - DB_PASSWORD=${DB_PASSWORD}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - KAFKA_SECURITY_PROTOCOL=${KAFKA_SECURITY_PROTOCOL}
      - KAFKA_SASL_USERNAME=${KAFKA_SASL_USERNAME}
      - KAFKA_SASL_PASSWORD=${KAFKA_SASL_PASSWORD}
      - KAFKA_CONSUMER_GROUP=${KAFKA_CONSUMER_GROUP}-processing
      - S3_REGION=${S3_REGION}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
      - SNOWFLAKE_WAREHOUSE=${SNOWFLAKE_WAREHOUSE}
      - SNOWFLAKE_DATABASE=${SNOWFLAKE_DATABASE}
      - SNOWFLAKE_USERNAME=${SNOWFLAKE_USERNAME}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
      - REDIS_URL=${REDIS_URL}
    volumes:
      - ../../logs:/app/logs
    restart: unless-stopped
    networks:
      - metrify-network
    command: ["python", "-m", "src.presentation.workers.processing_worker"]
    deploy:
      replicas: 2
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'

  # Metrify Monitoring Worker
  monitoring-worker:
    build:
      context: ../..
      dockerfile: infrastructure/docker/Dockerfile.worker
    container_name: metrify-monitoring-worker
    environment:
      - METRIFY_ENVIRONMENT=production
      - DB_HOST=${DB_HOST}
      - DB_PORT=5432
      - DB_NAME=${DB_NAME}
      - DB_USERNAME=${DB_USERNAME}
      - DB_PASSWORD=${DB_PASSWORD}
      - KAFKA_BOOTSTRAP_SERVERS=${KAFKA_BOOTSTRAP_SERVERS}
      - KAFKA_SECURITY_PROTOCOL=${KAFKA_SECURITY_PROTOCOL}
      - KAFKA_SASL_USERNAME=${KAFKA_SASL_USERNAME}
      - KAFKA_SASL_PASSWORD=${KAFKA_SASL_PASSWORD}
      - KAFKA_CONSUMER_GROUP=${KAFKA_CONSUMER_GROUP}-monitoring
      - S3_REGION=${S3_REGION}
      - S3_BUCKET_NAME=${S3_BUCKET_NAME}
      - S3_ACCESS_KEY_ID=${S3_ACCESS_KEY_ID}
      - S3_SECRET_ACCESS_KEY=${S3_SECRET_ACCESS_KEY}
      - DATADOG_API_KEY=${DATADOG_API_KEY}
      - DATADOG_APP_KEY=${DATADOG_APP_KEY}
      - REDIS_URL=${REDIS_URL}
    volumes:
      - ../../logs:/app/logs
    restart: unless-stopped
    networks:
      - metrify-network
    command: ["python", "-m", "src.presentation.workers.monitoring_worker"]
    deploy:
      replicas: 1
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

# Networks
networks:
  metrify-network:
    driver: bridge
    external: true
